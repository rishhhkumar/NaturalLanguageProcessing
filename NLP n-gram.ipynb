{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d3c7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a386a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rishikakumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c627c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Thank you so much for your help\",\n",
    "    \"I really appreciate your help\",\n",
    "    \"Excuse me, do you know what time it is\",\n",
    "    \"I’m really sorry for not inviting you\",\n",
    "    \"I really like your watch\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a131dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32c1420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'you': 3, 'your': 3, 'i': 3, 'really': 3, 'for': 2, 'help': 2, 'thank': 1, 'so': 1, 'much': 1, 'appreciate': 1, 'excuse': 1, 'me': 1, ',': 1, 'do': 1, 'know': 1, 'what': 1, 'time': 1, 'it': 1, 'is': 1, '’': 1, 'm': 1, 'sorry': 1, 'not': 1, 'inviting': 1, 'like': 1, 'watch': 1})\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [token for sentence in tokenized_corpus for token in sentence]\n",
    "\n",
    "token_freq = Counter(all_tokens)\n",
    "\n",
    "print(token_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb3268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = len(token_freq)\n",
    "\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f08c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('your', 'help'): 2, ('i', 'really'): 2, ('thank', 'you'): 1, ('you', 'so'): 1, ('so', 'much'): 1, ('much', 'for'): 1, ('for', 'your'): 1, ('really', 'appreciate'): 1, ('appreciate', 'your'): 1, ('excuse', 'me'): 1, ('me', ','): 1, (',', 'do'): 1, ('do', 'you'): 1, ('you', 'know'): 1, ('know', 'what'): 1, ('what', 'time'): 1, ('time', 'it'): 1, ('it', 'is'): 1, ('i', '’'): 1, ('’', 'm'): 1, ('m', 'really'): 1, ('really', 'sorry'): 1, ('sorry', 'for'): 1, ('for', 'not'): 1, ('not', 'inviting'): 1, ('inviting', 'you'): 1, ('really', 'like'): 1, ('like', 'your'): 1, ('your', 'watch'): 1})\n"
     ]
    }
   ],
   "source": [
    "all_bigrams = [bigram for sentence in tokenized_corpus for bigram in ngrams(sentence, 2)]\n",
    "\n",
    "bigram_freq = Counter(all_bigrams)\n",
    "\n",
    "print(bigram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "817a3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_probability(sentence, bigram_freq, token_freq, unique_tokens):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    bigrams = list(ngrams(tokens, 2))\n",
    "    \n",
    "    prob = 1.0\n",
    "    for bg in bigrams:            \n",
    "        n = bigram_freq.get(bg, 0) + 1\n",
    "        d = token_freq.get(bg[0], 0) + (unique_tokens)\n",
    "        prob *= (n / d)\n",
    "        \n",
    "    return f\"{prob:.10f}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "429ad958",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"I really like your garden\",\n",
    "    \"I really sorry for your garden\",\n",
    "    \"I really appropriate your garden\",\n",
    "    \"I really appreciate your garden\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52199d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of \"I really like your garden\": 0.0000182232\n",
      "Probability of \"I really sorry for your garden\": 0.0000013017\n",
      "Probability of \"I really appropriate your garden\": 0.0000047310\n",
      "Probability of \"I really appreciate your garden\": 0.0000182232\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    print(f\"Probability of \\\"{sentence}\\\": {sentence_probability(sentence, bigram_freq, token_freq, unique_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3941e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
